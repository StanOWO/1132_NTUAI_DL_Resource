{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vllzzAAIgi6x"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQqyEsqvko1r"
      },
      "source": [
        "## Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pdG96fGthn2S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTMUZMmgkqx5"
      },
      "source": [
        "## Check if GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "kilos5wzktSy",
        "outputId": "61cc292e-7e5e-4e89-882d-9d173c47334a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"Using GPU\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VST0A16bvOxL"
      },
      "source": [
        "## Kaggle Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l1Kz93eK1xaB"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir(\"1132_NTUAI_DL_Resource\"):\n",
        "  os.system(\"git clone https://github.com/StanOWO/1132_NTUAI_DL_Resource.git\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrMxyXfDhINZ"
      },
      "source": [
        "# Tensor Tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QCWE8ZZgKrE",
        "outputId": "3dd639c4-3bbf-450a-e458-a2a02d6f15c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor 1D: tensor([1., 2., 3.])\n",
            "torch.FloatTensor\n",
            "Tensor 2D: tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "torch.FloatTensor\n"
          ]
        }
      ],
      "source": [
        "# 1D Floating Point Tensor\n",
        "tensor_1D = torch.Tensor([1, 2, 3])\n",
        "print(\"Tensor 1D:\", tensor_1D)\n",
        "print(tensor_1D.type())\n",
        "\n",
        "# 2D Floating Point Tensor\n",
        "tensor_2D = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(\"Tensor 2D:\", tensor_2D)\n",
        "print(tensor_2D.type())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDVG5NEihuX0",
        "outputId": "6c2cb0a1-ceed-4328-e197-38d9f300574c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Addition: tensor([[7, 7, 7],\n",
            "        [7, 7, 7]])\n",
            "Subtraction: tensor([[-5, -3, -1],\n",
            "        [ 1,  3,  5]])\n",
            "Multiplication: tensor([[ 6, 10, 12],\n",
            "        [12, 10,  6]])\n",
            "Division: tensor([[0.1667, 0.4000, 0.7500],\n",
            "        [1.3333, 2.5000, 6.0000]])\n"
          ]
        }
      ],
      "source": [
        "# Tensor operation\n",
        "tensor_1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "tensor_2 = torch.tensor([[6, 5, 4], [3, 2, 1]])\n",
        "\n",
        "print(f\"Addition: {tensor_1 + tensor_2}\")\n",
        "print(f\"Subtraction: {tensor_1 - tensor_2}\")\n",
        "print(f\"Multiplication: {tensor_1 * tensor_2}\")\n",
        "print(f\"Division: {tensor_1 / tensor_2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vREgF1TziaHi",
        "outputId": "691793a1-29a1-4028-ab4f-5a6c89fb37bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor 1D: tensor([1, 2, 3])\n",
            "List 1D: [1, 2, 3]\n",
            "Tensor 1D: tensor([1, 2, 3])\n",
            "NDArray 1D: [1 2 3]\n"
          ]
        }
      ],
      "source": [
        "# Python List to Tensors\n",
        "list_1D = [1, 2, 3]\n",
        "tensor_1D = torch.tensor(list_1D)\n",
        "print(\"Tensor 1D:\", tensor_1D)\n",
        "\n",
        "# Tensors to Python List\n",
        "list_1D = tensor_1D.tolist()\n",
        "print(\"List 1D:\", list_1D)\n",
        "\n",
        "# NumPy NDArray to Tensors\n",
        "ndarray_1D = np.array([1, 2, 3])\n",
        "tensor_1D = torch.from_numpy(ndarray_1D)\n",
        "print(\"Tensor 1D:\", tensor_1D)\n",
        "\n",
        "# Tensors to NumPy NDArray\n",
        "ndarray_1D = tensor_1D.numpy()\n",
        "print(\"NDArray 1D:\", ndarray_1D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoYH_PlVlJs9",
        "outputId": "515f0496-f1fb-47fe-ae89-684522c7d793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor 1D: tensor([1, 2, 3])\n",
            "torch.LongTensor\n",
            "Tensor 1D GPU: tensor([1, 2, 3], device='cuda:0')\n",
            "torch.cuda.LongTensor\n",
            "Tensor 1D CPU: tensor([1, 2, 3])\n",
            "torch.LongTensor\n"
          ]
        }
      ],
      "source": [
        "tensor_1D = torch.tensor([1, 2, 3])\n",
        "print(\"Tensor 1D:\", tensor_1D)\n",
        "print(tensor_1D.type())\n",
        "\n",
        "# Transfer tensor from CPU to GPU\n",
        "tensor_1D_GPU = tensor_1D.to(device)\n",
        "print(\"Tensor 1D GPU:\", tensor_1D_GPU)\n",
        "print(tensor_1D_GPU.type())\n",
        "\n",
        "# Transfer tensor from GPU to CPU\n",
        "tensor_1D_CPU = tensor_1D_GPU.to(torch.device(\"cpu\"))\n",
        "print(\"Tensor 1D CPU:\", tensor_1D_CPU)\n",
        "print(tensor_1D_CPU.type())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQbrmK8Xkg-Z",
        "outputId": "841e5f08-2b5e-407f-a8a5-3584fdd90295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor 2D: tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "Axes: 2\n",
            "Shape: torch.Size([2, 3])\n",
            "Size: torch.Size([2, 3])\n",
            "Number of Elements: 6\n",
            "Data Type: torch.int64\n"
          ]
        }
      ],
      "source": [
        "# Get the “axes” of a tensor\n",
        "tensor_2D = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(\"Tensor 2D:\", tensor_2D)\n",
        "print(\"Axes:\", tensor_2D.dim())\n",
        "\n",
        "# Get the “dimension/shape” of a tensor\n",
        "print(\"Shape:\", tensor_2D.shape)\n",
        "print(\"Size:\", tensor_2D.size())\n",
        "\n",
        "# Get the “Number of Elements” of a tensor\n",
        "print(\"Number of Elements:\", tensor_2D.numel())\n",
        "\n",
        "# Get the “Data Type” of a tensor\n",
        "print(\"Data Type:\", tensor_2D.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jur6aSQg0M3s",
        "outputId": "99bea92b-e775-44fb-e73e-d80abcc02abb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor 2D: tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "Initial Shape: torch.Size([2, 3])\n",
            "Tensor after unsqueeze(dim=1): tensor([[[1, 2, 3]],\n",
            "\n",
            "        [[4, 5, 6]]])\n",
            "New Shape after unsqueeze: torch.Size([2, 1, 3])\n",
            "Tensor after squeeze(dim=1): tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "New Shape after squeeze: torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "tensor_2D = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(\"Tensor 2D:\", tensor_2D)\n",
        "print(\"Initial Shape:\", tensor_2D.shape)\n",
        "\n",
        "# Use unsqueeze to insert a new dimension at the specified dimension\n",
        "tensor_unsqueezed = tensor_2D.unsqueeze(dim=1)\n",
        "print(\"Tensor after unsqueeze(dim=1):\", tensor_unsqueezed)\n",
        "print(\"New Shape after unsqueeze:\", tensor_unsqueezed.size())\n",
        "\n",
        "# Use squeeze to remove dimensions of size 1\n",
        "tensor_squeezed = tensor_unsqueezed.squeeze(dim=1)\n",
        "print(\"Tensor after squeeze(dim=1):\", tensor_squeezed)\n",
        "print(\"New Shape after squeeze:\", tensor_squeezed.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m4PBde2j2sh"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVxVxYjx4NL5"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P27Z66Moj60T"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "dataset = load_diabetes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gO3JdraQ4sz2"
      },
      "outputs": [],
      "source": [
        "# Split independent variable and dependent variable\n",
        "X = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "Y = pd.DataFrame(dataset.target, columns=['Diabete_Value'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYQknvcx9i4z",
        "outputId": "b559c6be-8e24-4a28-dbb0-50434e753c04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        age       sex       bmi        bp        s1        s2        s3  \\\n",
            "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
            "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
            "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
            "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
            "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
            "5 -0.092695 -0.044642 -0.040696 -0.019442 -0.068991 -0.079288  0.041277   \n",
            "6 -0.045472  0.050680 -0.047163 -0.015999 -0.040096 -0.024800  0.000779   \n",
            "7  0.063504  0.050680 -0.001895  0.066629  0.090620  0.108914  0.022869   \n",
            "8  0.041708  0.050680  0.061696 -0.040099 -0.013953  0.006202 -0.028674   \n",
            "9 -0.070900 -0.044642  0.039062 -0.033213 -0.012577 -0.034508 -0.024993   \n",
            "\n",
            "         s4        s5        s6  \n",
            "0 -0.002592  0.019907 -0.017646  \n",
            "1 -0.039493 -0.068332 -0.092204  \n",
            "2 -0.002592  0.002861 -0.025930  \n",
            "3  0.034309  0.022688 -0.009362  \n",
            "4 -0.002592 -0.031988 -0.046641  \n",
            "5 -0.076395 -0.041176 -0.096346  \n",
            "6 -0.039493 -0.062917 -0.038357  \n",
            "7  0.017703 -0.035816  0.003064  \n",
            "8 -0.002592 -0.014960  0.011349  \n",
            "9 -0.002592  0.067737 -0.013504  \n",
            "   Diabete_Value\n",
            "0          151.0\n",
            "1           75.0\n",
            "2          141.0\n",
            "3          206.0\n",
            "4          135.0\n",
            "5           97.0\n",
            "6          138.0\n",
            "7           63.0\n",
            "8          110.0\n",
            "9          310.0\n"
          ]
        }
      ],
      "source": [
        "print(X.head(10))\n",
        "print(Y.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "817xJ2wD4tj5"
      },
      "outputs": [],
      "source": [
        "# Split train dataset and test dataset variable\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkwRs2QN4v6u"
      },
      "outputs": [],
      "source": [
        "# Transform into Tensor\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float)\n",
        "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.float)\n",
        "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI7tpfqB4QFb"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6BcRMmy4QcB"
      },
      "outputs": [],
      "source": [
        "class DiabeteModel(nn.Module):\n",
        "\n",
        "  # Define the architecture of each layer of the neural network\n",
        "  def __init__(self):\n",
        "    super(DiabeteModel, self).__init__()\n",
        "\n",
        "    # Define each neural layer\n",
        "    self.fc1 = nn.Linear(10, 20)\n",
        "    self.fc2 = nn.Linear(20, 10)\n",
        "    self.fc3 = nn.Linear(10, 1)\n",
        "\n",
        "    # Initialize the weights of each neural layer\n",
        "    init.xavier_normal_(self.fc1.weight)\n",
        "    init.xavier_normal_(self.fc2.weight)\n",
        "    init.xavier_normal_(self.fc3.weight)\n",
        "\n",
        "  # Define forward propagation\n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t6O5y7p59T6"
      },
      "outputs": [],
      "source": [
        "# Construct the model\n",
        "model = DiabeteModel().to(device)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U24jWIXX4QqM"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpjQU9tE9Lvm"
      },
      "outputs": [],
      "source": [
        "batch_size = 15\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5drg3xWS4TRx",
        "outputId": "0f9f1768-d6fd-4e19-dcdf-a6e7fc09d9b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/150,MSE Loss: 29164.7700\n",
            "Epoch 2/150,MSE Loss: 29115.1312\n",
            "Epoch 3/150,MSE Loss: 29062.5970\n",
            "Epoch 4/150,MSE Loss: 29001.4709\n",
            "Epoch 5/150,MSE Loss: 28918.5844\n",
            "Epoch 6/150,MSE Loss: 28811.9334\n",
            "Epoch 7/150,MSE Loss: 28671.4243\n",
            "Epoch 8/150,MSE Loss: 28493.6385\n",
            "Epoch 9/150,MSE Loss: 28267.1171\n",
            "Epoch 10/150,MSE Loss: 27982.1353\n",
            "Epoch 11/150,MSE Loss: 27615.9507\n",
            "Epoch 12/150,MSE Loss: 27171.8601\n",
            "Epoch 13/150,MSE Loss: 26626.2494\n",
            "Epoch 14/150,MSE Loss: 25996.3892\n",
            "Epoch 15/150,MSE Loss: 25249.3225\n",
            "Epoch 16/150,MSE Loss: 24381.7833\n",
            "Epoch 17/150,MSE Loss: 23390.7227\n",
            "Epoch 18/150,MSE Loss: 22309.7802\n",
            "Epoch 19/150,MSE Loss: 21125.9125\n",
            "Epoch 20/150,MSE Loss: 19897.3993\n",
            "Epoch 21/150,MSE Loss: 18578.8279\n",
            "Epoch 22/150,MSE Loss: 17253.8154\n",
            "Epoch 23/150,MSE Loss: 15914.4406\n",
            "Epoch 24/150,MSE Loss: 14563.1975\n",
            "Epoch 25/150,MSE Loss: 13251.4255\n",
            "Epoch 26/150,MSE Loss: 11994.0963\n",
            "Epoch 27/150,MSE Loss: 10824.8221\n",
            "Epoch 28/150,MSE Loss: 9711.0549\n",
            "Epoch 29/150,MSE Loss: 8731.9248\n",
            "Epoch 30/150,MSE Loss: 7856.2922\n",
            "Epoch 31/150,MSE Loss: 7088.6365\n",
            "Epoch 32/150,MSE Loss: 6456.2605\n",
            "Epoch 33/150,MSE Loss: 5912.1532\n",
            "Epoch 34/150,MSE Loss: 5473.8239\n",
            "Epoch 35/150,MSE Loss: 5146.5625\n",
            "Epoch 36/150,MSE Loss: 4872.3334\n",
            "Epoch 37/150,MSE Loss: 4671.7089\n",
            "Epoch 38/150,MSE Loss: 4525.0411\n",
            "Epoch 39/150,MSE Loss: 4398.9802\n",
            "Epoch 40/150,MSE Loss: 4313.2596\n",
            "Epoch 41/150,MSE Loss: 4251.0120\n",
            "Epoch 42/150,MSE Loss: 4200.0458\n",
            "Epoch 43/150,MSE Loss: 4152.4776\n",
            "Epoch 44/150,MSE Loss: 4113.0818\n",
            "Epoch 45/150,MSE Loss: 4080.3207\n",
            "Epoch 46/150,MSE Loss: 4048.6544\n",
            "Epoch 47/150,MSE Loss: 4021.0736\n",
            "Epoch 48/150,MSE Loss: 3993.3961\n",
            "Epoch 49/150,MSE Loss: 3966.0867\n",
            "Epoch 50/150,MSE Loss: 3940.1638\n",
            "Epoch 51/150,MSE Loss: 3915.1015\n",
            "Epoch 52/150,MSE Loss: 3886.8238\n",
            "Epoch 53/150,MSE Loss: 3854.6714\n",
            "Epoch 54/150,MSE Loss: 3826.0077\n",
            "Epoch 55/150,MSE Loss: 3798.5163\n",
            "Epoch 56/150,MSE Loss: 3773.5697\n",
            "Epoch 57/150,MSE Loss: 3747.5195\n",
            "Epoch 58/150,MSE Loss: 3722.8626\n",
            "Epoch 59/150,MSE Loss: 3699.8195\n",
            "Epoch 60/150,MSE Loss: 3675.2225\n",
            "Epoch 61/150,MSE Loss: 3651.4610\n",
            "Epoch 62/150,MSE Loss: 3628.9508\n",
            "Epoch 63/150,MSE Loss: 3607.1982\n",
            "Epoch 64/150,MSE Loss: 3585.0670\n",
            "Epoch 65/150,MSE Loss: 3564.9171\n",
            "Epoch 66/150,MSE Loss: 3545.5612\n",
            "Epoch 67/150,MSE Loss: 3523.8442\n",
            "Epoch 68/150,MSE Loss: 3503.6694\n",
            "Epoch 69/150,MSE Loss: 3485.2278\n",
            "Epoch 70/150,MSE Loss: 3468.3960\n",
            "Epoch 71/150,MSE Loss: 3448.3897\n",
            "Epoch 72/150,MSE Loss: 3429.4839\n",
            "Epoch 73/150,MSE Loss: 3413.6883\n",
            "Epoch 74/150,MSE Loss: 3395.7890\n",
            "Epoch 75/150,MSE Loss: 3380.6098\n",
            "Epoch 76/150,MSE Loss: 3363.9309\n",
            "Epoch 77/150,MSE Loss: 3350.7392\n",
            "Epoch 78/150,MSE Loss: 3337.1410\n",
            "Epoch 79/150,MSE Loss: 3320.6831\n",
            "Epoch 80/150,MSE Loss: 3310.2134\n",
            "Epoch 81/150,MSE Loss: 3291.5318\n",
            "Epoch 82/150,MSE Loss: 3283.8544\n",
            "Epoch 83/150,MSE Loss: 3266.1809\n",
            "Epoch 84/150,MSE Loss: 3251.9871\n",
            "Epoch 85/150,MSE Loss: 3243.8938\n",
            "Epoch 86/150,MSE Loss: 3229.4034\n",
            "Epoch 87/150,MSE Loss: 3216.3241\n",
            "Epoch 88/150,MSE Loss: 3205.4085\n",
            "Epoch 89/150,MSE Loss: 3193.7456\n",
            "Epoch 90/150,MSE Loss: 3182.5632\n",
            "Epoch 91/150,MSE Loss: 3171.7480\n",
            "Epoch 92/150,MSE Loss: 3165.1599\n",
            "Epoch 93/150,MSE Loss: 3151.6233\n",
            "Epoch 94/150,MSE Loss: 3144.2415\n",
            "Epoch 95/150,MSE Loss: 3132.6434\n",
            "Epoch 96/150,MSE Loss: 3122.4470\n",
            "Epoch 97/150,MSE Loss: 3112.7486\n",
            "Epoch 98/150,MSE Loss: 3102.8769\n",
            "Epoch 99/150,MSE Loss: 3095.8320\n",
            "Epoch 100/150,MSE Loss: 3085.8347\n",
            "Epoch 101/150,MSE Loss: 3077.3627\n",
            "Epoch 102/150,MSE Loss: 3071.1688\n",
            "Epoch 103/150,MSE Loss: 3062.5446\n",
            "Epoch 104/150,MSE Loss: 3051.7835\n",
            "Epoch 105/150,MSE Loss: 3045.9415\n",
            "Epoch 106/150,MSE Loss: 3036.1129\n",
            "Epoch 107/150,MSE Loss: 3030.4998\n",
            "Epoch 108/150,MSE Loss: 3022.9317\n",
            "Epoch 109/150,MSE Loss: 3014.7852\n",
            "Epoch 110/150,MSE Loss: 3008.3688\n",
            "Epoch 111/150,MSE Loss: 3003.0999\n",
            "Epoch 112/150,MSE Loss: 2994.2020\n",
            "Epoch 113/150,MSE Loss: 2990.3094\n",
            "Epoch 114/150,MSE Loss: 2981.6579\n",
            "Epoch 115/150,MSE Loss: 2977.4809\n",
            "Epoch 116/150,MSE Loss: 2967.7119\n",
            "Epoch 117/150,MSE Loss: 2963.2661\n",
            "Epoch 118/150,MSE Loss: 2956.8960\n",
            "Epoch 119/150,MSE Loss: 2952.2123\n",
            "Epoch 120/150,MSE Loss: 2947.7967\n",
            "Epoch 121/150,MSE Loss: 2939.9763\n",
            "Epoch 122/150,MSE Loss: 2936.2289\n",
            "Epoch 123/150,MSE Loss: 2931.5115\n",
            "Epoch 124/150,MSE Loss: 2926.3510\n",
            "Epoch 125/150,MSE Loss: 2920.2248\n",
            "Epoch 126/150,MSE Loss: 2915.3058\n",
            "Epoch 127/150,MSE Loss: 2910.5527\n",
            "Epoch 128/150,MSE Loss: 2909.7760\n",
            "Epoch 129/150,MSE Loss: 2903.5523\n",
            "Epoch 130/150,MSE Loss: 2898.0003\n",
            "Epoch 131/150,MSE Loss: 2895.7506\n",
            "Epoch 132/150,MSE Loss: 2891.3284\n",
            "Epoch 133/150,MSE Loss: 2888.9152\n",
            "Epoch 134/150,MSE Loss: 2881.1373\n",
            "Epoch 135/150,MSE Loss: 2890.3200\n",
            "Epoch 136/150,MSE Loss: 2875.7672\n",
            "Epoch 137/150,MSE Loss: 2870.4096\n",
            "Epoch 138/150,MSE Loss: 2867.2778\n",
            "Epoch 139/150,MSE Loss: 2866.4077\n",
            "Epoch 140/150,MSE Loss: 2865.4178\n",
            "Epoch 141/150,MSE Loss: 2857.2305\n",
            "Epoch 142/150,MSE Loss: 2858.0852\n",
            "Epoch 143/150,MSE Loss: 2850.8291\n",
            "Epoch 144/150,MSE Loss: 2853.8527\n",
            "Epoch 145/150,MSE Loss: 2844.8332\n",
            "Epoch 146/150,MSE Loss: 2843.7425\n",
            "Epoch 147/150,MSE Loss: 2840.8126\n",
            "Epoch 148/150,MSE Loss: 2839.5439\n",
            "Epoch 149/150,MSE Loss: 2835.4066\n",
            "Epoch 150/150,MSE Loss: 2832.3476\n"
          ]
        }
      ],
      "source": [
        "epochs = 150\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # Set the model to training mode\n",
        "  model.train()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  total_samples = 0\n",
        "\n",
        "  # Take a Batch and start training\n",
        "  for X_batch, Y_batch in train_loader:\n",
        "    # Translate the data from the Batch to the GPU\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "    # Zero out the gradient of the previous Batch\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate the output\n",
        "    Y_pred = model(X_batch)\n",
        "    # Calculate the loss\n",
        "    loss = criterion(Y_pred, Y_batch)\n",
        "    # Backward propagation\n",
        "    loss.backward()\n",
        "    # Update the weight\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item() * X_batch.size(0)\n",
        "    total_samples += X_batch.size(0)\n",
        "\n",
        "  # Calculate the accuracy for each epoch\n",
        "  epoch_loss = running_loss / total_samples\n",
        "  print(f'Epoch {epoch+1}/{epochs},MSE Loss: {epoch_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAFsXr9c4VRo"
      },
      "source": [
        "## Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsMhCZ3Y4VvV",
        "outputId": "ea964455-4c8c-40b5-8e1f-37ae22fd0b22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test MSE Loss: 3293.3597\n"
          ]
        }
      ],
      "source": [
        "# Switch to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "test_loss = 0.0\n",
        "total_samples = 0\n",
        "\n",
        "# Turn off PyTorch's gradient calculation\n",
        "with torch.no_grad():\n",
        "  # Take a Batch and start testing\n",
        "  for X_batch, Y_batch in test_loader:\n",
        "    # Transform the data from CPU into GPU.\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "    # Calculate output values\n",
        "    Y_pred = model(X_batch)\n",
        "\n",
        "    # Calculate model predictions\n",
        "    Y_pred = model(X_batch)\n",
        "    # Calculate loss for the batch\n",
        "    loss = criterion(Y_pred, Y_batch)\n",
        "\n",
        "    # Accumulate total loss and sample count\n",
        "    test_loss += loss.item() * X_batch.size(0)\n",
        "    total_samples += X_batch.size(0)\n",
        "\n",
        "# Compute the average loss for the test dataset\n",
        "avg_test_loss = test_loss / total_samples\n",
        "print(f'Test MSE Loss: {avg_test_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TWsKMM-mq_7"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKE9BS03m58v"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgx-5isdmwfJ"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "dataset = load_iris()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvsG3olCm7gu"
      },
      "outputs": [],
      "source": [
        "# Split independent variable and dependent variable\n",
        "X = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "Y = pd.DataFrame(dataset.target, columns=['Iris_Type'])\n",
        "Y_name = dataset.target_names.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMR5O3DHnorB"
      },
      "outputs": [],
      "source": [
        "# Split train dataset and test dataset variable\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDK18v52np22"
      },
      "outputs": [],
      "source": [
        "# Feature Scaling\n",
        "sc_X = StandardScaler().fit(X_train)\n",
        "X_train = sc_X.transform(X_train)\n",
        "X_test = sc_X.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqwEHbibnutg"
      },
      "outputs": [],
      "source": [
        "# Transform into Tensor\n",
        "X_train_tensor = torch.from_numpy(X_train).float()\n",
        "X_test_tensor = torch.from_numpy(X_test).float()\n",
        "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.long)\n",
        "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmrQ8Cv4owUF"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qibmiQM0o0Ka"
      },
      "outputs": [],
      "source": [
        "class IrisModel(nn.Module):\n",
        "\n",
        "  # Define the architecture of each layer of the neural network\n",
        "  def __init__(self):\n",
        "    super(IrisModel, self).__init__()\n",
        "\n",
        "    # Define each neural layer\n",
        "    self.fc1 = nn.Linear(4, 16)\n",
        "    self.fc2 = nn.Linear(16, 16)\n",
        "    self.fc3 = nn.Linear(16, 3)\n",
        "\n",
        "    # Initialize the weights of each neural layer\n",
        "    init.xavier_normal_(self.fc1.weight)\n",
        "    init.xavier_normal_(self.fc2.weight)\n",
        "    init.xavier_normal_(self.fc3.weight)\n",
        "\n",
        "  # Define forward propagation\n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6TptxlHpmp2"
      },
      "outputs": [],
      "source": [
        "# Construct the model\n",
        "model = IrisModel().to(device)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfkBK2Ktp7Kc"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzGQ6qc4qFe6"
      },
      "outputs": [],
      "source": [
        "batch_size = 15\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBv8HtMx8uHn",
        "outputId": "6b80d9ec-a97c-409e-ab34-e86a9213d184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Loss: 1.1235, Acc: 0.2583\n",
            "Epoch 2/50, Loss: 1.0572, Acc: 0.2917\n",
            "Epoch 3/50, Loss: 0.9838, Acc: 0.3500\n",
            "Epoch 4/50, Loss: 0.8840, Acc: 0.6167\n",
            "Epoch 5/50, Loss: 0.8863, Acc: 0.7333\n",
            "Epoch 6/50, Loss: 0.8835, Acc: 0.7667\n",
            "Epoch 7/50, Loss: 0.9135, Acc: 0.7750\n",
            "Epoch 8/50, Loss: 0.8711, Acc: 0.8000\n",
            "Epoch 9/50, Loss: 0.7784, Acc: 0.8333\n",
            "Epoch 10/50, Loss: 0.6622, Acc: 0.8417\n",
            "Epoch 11/50, Loss: 0.6265, Acc: 0.8583\n",
            "Epoch 12/50, Loss: 0.5687, Acc: 0.8583\n",
            "Epoch 13/50, Loss: 0.4923, Acc: 0.8583\n",
            "Epoch 14/50, Loss: 0.4652, Acc: 0.8500\n",
            "Epoch 15/50, Loss: 0.5346, Acc: 0.8667\n",
            "Epoch 16/50, Loss: 0.5023, Acc: 0.8667\n",
            "Epoch 17/50, Loss: 0.3713, Acc: 0.8667\n",
            "Epoch 18/50, Loss: 0.4078, Acc: 0.8750\n",
            "Epoch 19/50, Loss: 0.4822, Acc: 0.8750\n",
            "Epoch 20/50, Loss: 0.3064, Acc: 0.8667\n",
            "Epoch 21/50, Loss: 0.3238, Acc: 0.8667\n",
            "Epoch 22/50, Loss: 0.3014, Acc: 0.8750\n",
            "Epoch 23/50, Loss: 0.3410, Acc: 0.8750\n",
            "Epoch 24/50, Loss: 0.3723, Acc: 0.8750\n",
            "Epoch 25/50, Loss: 0.4348, Acc: 0.8833\n",
            "Epoch 26/50, Loss: 0.2991, Acc: 0.8917\n",
            "Epoch 27/50, Loss: 0.4205, Acc: 0.8917\n",
            "Epoch 28/50, Loss: 0.3373, Acc: 0.8917\n",
            "Epoch 29/50, Loss: 0.2921, Acc: 0.8917\n",
            "Epoch 30/50, Loss: 0.2758, Acc: 0.8917\n",
            "Epoch 31/50, Loss: 0.1669, Acc: 0.8917\n",
            "Epoch 32/50, Loss: 0.2224, Acc: 0.9083\n",
            "Epoch 33/50, Loss: 0.3408, Acc: 0.9083\n",
            "Epoch 34/50, Loss: 0.4941, Acc: 0.9083\n",
            "Epoch 35/50, Loss: 0.3708, Acc: 0.9083\n",
            "Epoch 36/50, Loss: 0.1588, Acc: 0.9083\n",
            "Epoch 37/50, Loss: 0.2760, Acc: 0.9083\n",
            "Epoch 38/50, Loss: 0.2688, Acc: 0.9167\n",
            "Epoch 39/50, Loss: 0.1624, Acc: 0.9333\n",
            "Epoch 40/50, Loss: 0.2238, Acc: 0.9417\n",
            "Epoch 41/50, Loss: 0.2641, Acc: 0.9500\n",
            "Epoch 42/50, Loss: 0.2176, Acc: 0.9500\n",
            "Epoch 43/50, Loss: 0.2321, Acc: 0.9500\n",
            "Epoch 44/50, Loss: 0.1464, Acc: 0.9500\n",
            "Epoch 45/50, Loss: 0.1073, Acc: 0.9500\n",
            "Epoch 46/50, Loss: 0.1089, Acc: 0.9500\n",
            "Epoch 47/50, Loss: 0.1512, Acc: 0.9500\n",
            "Epoch 48/50, Loss: 0.1684, Acc: 0.9500\n",
            "Epoch 49/50, Loss: 0.1424, Acc: 0.9500\n",
            "Epoch 50/50, Loss: 0.1426, Acc: 0.9500\n"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # Set the model to training mode\n",
        "  model.train()\n",
        "\n",
        "  # Store the number of correct guesses & the full number of guesses\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  # Take a Batch and start training\n",
        "  for X_batch, Y_batch in train_loader:\n",
        "    # Translate the data from the Batch to the GPU\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "    # Zero out the gradient of the previous Batch\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate the output\n",
        "    Y_pred = model(X_batch)\n",
        "    # Calculate the loss\n",
        "    loss = criterion(Y_pred, Y_batch.squeeze())\n",
        "    # Backward propagation\n",
        "    loss.backward()\n",
        "    # Update the weight\n",
        "    optimizer.step()\n",
        "\n",
        "    # Find the biggest index\n",
        "    _, predicted = torch.max(Y_pred.data, 1)\n",
        "    # Add the data of this batch to total\n",
        "    total += Y_batch.size(0)\n",
        "    # Calculate the number of correct guesses (.item() will help get the pure amount)\n",
        "    correct += (predicted == Y_batch.squeeze()).sum().item()\n",
        "\n",
        "    # Calculate the accuracy for each epoch\n",
        "    accuracy = correct / total\n",
        "  print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Acc: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FWBukCRqJ-B"
      },
      "outputs": [],
      "source": [
        "# epochs = 100\n",
        "\n",
        "# acc_hist = []\n",
        "# loss_hist = []\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#   # Set the model to training mode\n",
        "#   model.train()\n",
        "\n",
        "#   # Store the number of correct guesses & the full number of guesses\n",
        "#   correct = 0\n",
        "#   total = 0\n",
        "\n",
        "#   # Create a progress bar for the current epoch.\n",
        "#   batch_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\")\n",
        "#   # Take a Batch and start training\n",
        "#   for X_batch, Y_batch in batch_bar:\n",
        "#     # Translate the data from the Batch to the GPU\n",
        "#     X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "#     # Zero out the gradient of the previous Batch\n",
        "#     optimizer.zero_grad()\n",
        "\n",
        "#     # Calculate the output\n",
        "#     Y_pred = model(X_batch)\n",
        "#     # Calculate the loss\n",
        "#     loss = criterion(Y_pred, Y_batch.squeeze())\n",
        "#     # Backward propagation\n",
        "#     loss.backward()\n",
        "#     # Update the weight\n",
        "#     optimizer.step()\n",
        "\n",
        "#     # Find the biggest perbentage of index\n",
        "#     _, predicted = torch.max(Y_pred.data, 1)\n",
        "#     # Add the data of this batch to total\n",
        "#     total += Y_batch.size(0)\n",
        "#     # Calculate the number of correct guesses\n",
        "#     correct += (predicted == Y_batch.squeeze()).sum().item()\n",
        "\n",
        "#     # Calculate the accuracy\n",
        "#     accuracy = correct / total\n",
        "\n",
        "#     # Updating the information on the progress bar\n",
        "#     batch_bar.set_postfix({'Loss': f'{loss.item():.4f}', 'Accuracy': f'{accuracy:.4f}'})\n",
        "#     batch_bar.refresh()\n",
        "\n",
        "#   acc_hist.append(accuracy)\n",
        "#   loss_hist.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztJPuEVny8vu"
      },
      "outputs": [],
      "source": [
        "# def plot_history(acc_hist, loss_hist):\n",
        "#     epochs_range = range(1, len(acc_hist) + 1)\n",
        "\n",
        "#     fig, ax1 = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "#     color = 'tab:red'\n",
        "#     ax1.set_xlabel('Epoch')\n",
        "#     ax1.set_ylabel('Accuracy', color=color)\n",
        "#     ax1.plot(epochs_range, acc_hist, color=color, label=\"Accuracy\")\n",
        "#     ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "#     ax2 = ax1.twinx()\n",
        "#     color = 'tab:blue'\n",
        "#     ax2.set_ylabel('Loss', color=color)\n",
        "#     ax2.plot(epochs_range, loss_hist, color=color, label=\"Loss\")\n",
        "#     ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "#     fig.tight_layout()\n",
        "#     plt.title(\"Training Accuracy and Loss\")\n",
        "#     plt.show()\n",
        "\n",
        "# plot_history(acc_hist, loss_hist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_1GLoIYqDbT"
      },
      "source": [
        "## Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZXfguv8qFBL",
        "outputId": "ba06d05a-dd59-47b6-a2eb-f0559f89bce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 90.00%\n"
          ]
        }
      ],
      "source": [
        "# Switch to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Turn off PyTorch's gradient calculation\n",
        "with torch.no_grad():\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  # Take a Batch and start testing\n",
        "  for X_batch, Y_batch in test_loader:\n",
        "    # Transform the data from CPU into GPU.\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "    # Calculate output values\n",
        "    Y_pred = model(X_batch)\n",
        "\n",
        "    # Find the biggest perbentage of index\n",
        "    _, predicted = torch.max(Y_pred.data, 1)\n",
        "    # Add the data of this batch to total\n",
        "    total += Y_batch.size(0)\n",
        "    # Calculate the number of correct guesses\n",
        "    correct += (predicted == Y_batch.squeeze()).sum().item()\n",
        "\n",
        "  print(f'Test Accuracy: {correct / total:.2%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3v-bkvUtaj8"
      },
      "source": [
        "# Kaggle Competition(Titanic)\n",
        "\n",
        "[連結](https://www.kaggle.com/competitions/titanic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXlsyviGv6en"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piDUEX4cv6en"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "\n",
        "dataset = pd.read_csv('/content/1132_NTUAI_DL_Resource/titanic/train.csv')\n",
        "dataset_test = pd.read_csv('/content/1132_NTUAI_DL_Resource/titanic/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34Vvyj_fmY96"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.dropna(subset=['Embarked'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyIxcgb_v6en"
      },
      "outputs": [],
      "source": [
        "# Split independent variable and dependent variable\n",
        "X = dataset.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin', 'Survived', 'Age'])\n",
        "Y = pd.DataFrame(dataset, columns=['Survived'])\n",
        "X_pred = dataset_test.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin', 'Age'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLwBGbkMxuz4"
      },
      "outputs": [],
      "source": [
        "# One-Hot encoder\n",
        "X_mod = pd.get_dummies(X, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "X_pred_mod = pd.get_dummies(X_pred, columns=['Sex', 'Embarked'], drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knAcMNKkv6en"
      },
      "outputs": [],
      "source": [
        "# Split train dataset and test dataset variable\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_mod, Y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BicOD1kv6en"
      },
      "outputs": [],
      "source": [
        "# Feature Scaling\n",
        "sc_X = StandardScaler().fit(X_train)\n",
        "X_train = sc_X.transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        "X_pred_scale = sc_X.transform(X_pred_mod)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVJT6xL-v6eo"
      },
      "outputs": [],
      "source": [
        "# Transform into Tensor\n",
        "X_train_tensor = torch.from_numpy(X_train).float()\n",
        "X_test_tensor = torch.from_numpy(X_test).float()\n",
        "X_pred_tensor = torch.from_numpy(X_pred_scale).float()\n",
        "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.float)\n",
        "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Em6gjj3v6eo"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDOpJTaSv6eo"
      },
      "outputs": [],
      "source": [
        "class TitanicModel(nn.Module):\n",
        "\n",
        "  # Define the architecture of each layer of the neural network\n",
        "  def __init__(self, input_len):\n",
        "    super(TitanicModel, self).__init__()\n",
        "\n",
        "    # Define each neural layer\n",
        "    self.fc1 = nn.Linear(input_len, 13)\n",
        "    self.fc2 = nn.Linear(13, 13)\n",
        "    self.fc3 = nn.Linear(13, 1)\n",
        "\n",
        "    # Initialize the weights of each neural layer\n",
        "    init.xavier_normal_(self.fc1.weight)\n",
        "    init.xavier_normal_(self.fc2.weight)\n",
        "    init.xavier_normal_(self.fc3.weight)\n",
        "\n",
        "  # Define forward propagation\n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = torch.sigmoid(self.fc3(x))\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMS8iP3Tv6eo"
      },
      "outputs": [],
      "source": [
        "# Construct the model\n",
        "model = TitanicModel(X_train_tensor.shape[1]).to(device)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBi1IwmIv6eo"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9StQDlQv6eo"
      },
      "outputs": [],
      "source": [
        "batch_size = 15\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqQcCWgov6eo",
        "outputId": "c0360ce6-019d-46d0-dba1-4200af519f10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Loss: 0.6352, Acc: 0.5556\n",
            "Epoch 2/50, Loss: 0.4765, Acc: 0.7103\n",
            "Epoch 3/50, Loss: 0.5724, Acc: 0.6962\n",
            "Epoch 4/50, Loss: 0.7052, Acc: 0.7482\n",
            "Epoch 5/50, Loss: 0.3761, Acc: 0.7904\n",
            "Epoch 6/50, Loss: 0.5385, Acc: 0.8200\n",
            "Epoch 7/50, Loss: 0.4009, Acc: 0.8186\n",
            "Epoch 8/50, Loss: 0.4260, Acc: 0.8143\n",
            "Epoch 9/50, Loss: 0.5415, Acc: 0.8087\n",
            "Epoch 10/50, Loss: 0.7234, Acc: 0.8073\n",
            "Epoch 11/50, Loss: 1.1844, Acc: 0.8101\n",
            "Epoch 12/50, Loss: 0.1025, Acc: 0.8087\n",
            "Epoch 13/50, Loss: 1.1056, Acc: 0.8101\n",
            "Epoch 14/50, Loss: 0.7864, Acc: 0.8158\n",
            "Epoch 15/50, Loss: 0.3053, Acc: 0.8101\n",
            "Epoch 16/50, Loss: 0.3753, Acc: 0.8087\n",
            "Epoch 17/50, Loss: 0.5023, Acc: 0.8200\n",
            "Epoch 18/50, Loss: 0.1980, Acc: 0.8186\n",
            "Epoch 19/50, Loss: 0.4698, Acc: 0.8214\n",
            "Epoch 20/50, Loss: 0.8276, Acc: 0.8214\n",
            "Epoch 21/50, Loss: 0.2015, Acc: 0.8228\n",
            "Epoch 22/50, Loss: 0.2550, Acc: 0.8256\n",
            "Epoch 23/50, Loss: 0.6540, Acc: 0.8172\n",
            "Epoch 24/50, Loss: 0.0936, Acc: 0.8298\n",
            "Epoch 25/50, Loss: 0.1080, Acc: 0.8383\n",
            "Epoch 26/50, Loss: 0.7144, Acc: 0.8368\n",
            "Epoch 27/50, Loss: 0.2608, Acc: 0.8368\n",
            "Epoch 28/50, Loss: 0.3467, Acc: 0.8354\n",
            "Epoch 29/50, Loss: 0.8055, Acc: 0.8411\n",
            "Epoch 30/50, Loss: 0.4222, Acc: 0.8383\n",
            "Epoch 31/50, Loss: 0.3853, Acc: 0.8397\n",
            "Epoch 32/50, Loss: 0.2342, Acc: 0.8397\n",
            "Epoch 33/50, Loss: 0.5943, Acc: 0.8439\n",
            "Epoch 34/50, Loss: 0.5246, Acc: 0.8368\n",
            "Epoch 35/50, Loss: 0.2768, Acc: 0.8383\n",
            "Epoch 36/50, Loss: 0.2928, Acc: 0.8411\n",
            "Epoch 37/50, Loss: 0.6281, Acc: 0.8453\n",
            "Epoch 38/50, Loss: 0.7520, Acc: 0.8453\n",
            "Epoch 39/50, Loss: 0.3981, Acc: 0.8453\n",
            "Epoch 40/50, Loss: 0.6034, Acc: 0.8481\n",
            "Epoch 41/50, Loss: 0.1802, Acc: 0.8411\n",
            "Epoch 42/50, Loss: 0.2659, Acc: 0.8439\n",
            "Epoch 43/50, Loss: 0.5901, Acc: 0.8467\n",
            "Epoch 44/50, Loss: 0.2018, Acc: 0.8439\n",
            "Epoch 45/50, Loss: 0.6142, Acc: 0.8453\n",
            "Epoch 46/50, Loss: 0.2658, Acc: 0.8453\n",
            "Epoch 47/50, Loss: 0.3154, Acc: 0.8467\n",
            "Epoch 48/50, Loss: 0.3701, Acc: 0.8453\n",
            "Epoch 49/50, Loss: 0.4778, Acc: 0.8467\n",
            "Epoch 50/50, Loss: 0.4226, Acc: 0.8481\n"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # Set the model to training mode\n",
        "  model.train()\n",
        "\n",
        "  # Store the number of correct guesses & the full number of guesses\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  # Take a Batch and start training\n",
        "  for X_batch, Y_batch in train_loader:\n",
        "    # Translate the data from the Batch to the GPU\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "    # Zero out the gradient of the previous Batch\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate the output\n",
        "    Y_pred = model(X_batch)\n",
        "    # Calculate the loss\n",
        "    loss = criterion(Y_pred.squeeze(), Y_batch.squeeze())\n",
        "    # Backward propagation\n",
        "    loss.backward()\n",
        "    # Update the weight\n",
        "    optimizer.step()\n",
        "\n",
        "    # Add the data of this batch to total\n",
        "    total += Y_batch.size(0)\n",
        "\n",
        "    # Calculate the number of correct guesses (.item() will help get the pure amount)\n",
        "    predicted = (Y_pred.squeeze() >= 0.5).float()\n",
        "    correct += (predicted == Y_batch.squeeze()).sum().item()\n",
        "\n",
        "  # Calculate the accuracy for each epoch\n",
        "  accuracy = correct / total\n",
        "  print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Acc: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6j2XXb9v6eo"
      },
      "source": [
        "## Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t75g5E2iv6eo",
        "outputId": "00167b99-fda0-484a-c84c-879e9d9da7c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 73.03%\n"
          ]
        }
      ],
      "source": [
        "# Switch to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Turn off PyTorch's gradient calculation\n",
        "with torch.no_grad():\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  # Take a Batch and start testing\n",
        "  for X_batch, Y_batch in test_loader:\n",
        "    # Transform the data from CPU into GPU.\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "    # Calculate output values\n",
        "    Y_pred = model(X_batch)\n",
        "\n",
        "    # Find the biggest perbentage of index\n",
        "    predicted = (Y_pred.squeeze() >= 0.5).float()\n",
        "\n",
        "    # Add the data of this batch to total\n",
        "    total += Y_batch.size(0)\n",
        "    # Calculate the number of correct guesses\n",
        "    correct += (predicted == Y_batch.squeeze()).sum().item()\n",
        "\n",
        "  print(f'Test Accuracy: {correct / total:.2%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb2C01lyg4a3"
      },
      "source": [
        "## Predict Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q07cCR20g9_e"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    Y_pred = model(X_pred_tensor.to(device))\n",
        "\n",
        "    Y_pred_label = (Y_pred >= 0.5).long()\n",
        "\n",
        "Y_pred_numpy = Y_pred_label.squeeze().cpu().tolist()\n",
        "\n",
        "submission = pd.DataFrame({'PassengerId': dataset_test['PassengerId'], 'Survived': Y_pred_numpy})\n",
        "submission.to_csv('submission_titanic.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIysAAnsgteG"
      },
      "source": [
        "# Kaggke Competition(House Prices)\n",
        "[連結](https://www.kaggle.com/competitions/titanic/data?select=test.csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXFpMP-ojrhR"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SRJIZfS-jrhR"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "\n",
        "dataset = pd.read_csv('/content/1132_NTUAI_DL_Resource/house-prices-advanced-regression-techniques/train.csv')\n",
        "dataset_test = pd.read_csv('/content/1132_NTUAI_DL_Resource//house-prices-advanced-regression-techniques/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GoPrxZaBjrhR"
      },
      "outputs": [],
      "source": [
        "# Split independent variable and dependent variable\n",
        "X = pd.DataFrame(dataset, columns=['MSSubClass', 'MSZoning', 'LotArea', 'OverallQual', 'OverallCond', 'CentralAir', 'YrSold', 'SaleType', 'SaleCondition'])\n",
        "Y = pd.DataFrame(dataset, columns=['SalePrice'])\n",
        "X_pred = pd.DataFrame(dataset_test, columns=['MSSubClass', 'MSZoning', 'LotArea', 'OverallQual', 'OverallCond', 'CentralAir', 'YrSold', 'SaleType', 'SaleCondition'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ql-KkueoRt9",
        "outputId": "e012723c-0e07-4918-8673-86a2e69bb51b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [MSSubClass, MSZoning, LotArea, OverallQual, OverallCond, CentralAir, YrSold, SaleType, SaleCondition]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "rows_with_nan = X[X.isna().any(axis=1)]\n",
        "print(rows_with_nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZUiRaT6Jm3OS"
      },
      "outputs": [],
      "source": [
        "# One-Hot encoder\n",
        "X_mod = pd.get_dummies(X, columns=['MSZoning', 'CentralAir', 'SaleType', 'SaleCondition'], drop_first=True).astype(float)\n",
        "X_pred_mod = pd.get_dummies(X_pred, columns=['MSZoning', 'CentralAir', 'SaleType', 'SaleCondition'], drop_first=True).astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w1_qbPUJjrhR"
      },
      "outputs": [],
      "source": [
        "# Split train dataset and test dataset variable\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_mod, Y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JR7JuxQKjrhR"
      },
      "outputs": [],
      "source": [
        "# Transform into Tensor\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float)\n",
        "X_pred_tensor = torch.tensor(X_pred_mod.values, dtype=torch.float)\n",
        "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.float)\n",
        "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDL3hvyFjrhR"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XY5gspSIjrhR"
      },
      "outputs": [],
      "source": [
        "class PriceModel(nn.Module):\n",
        "\n",
        "  # Define the architecture of each layer of the neural network\n",
        "  def __init__(self, input_len):\n",
        "    super(PriceModel, self).__init__()\n",
        "\n",
        "    # Define each neural layer\n",
        "    self.fc1 = nn.Linear(input_len, 61)\n",
        "    self.fc2 = nn.Linear(61, 30)\n",
        "    self.fc3 = nn.Linear(30, 1)\n",
        "\n",
        "    # Initialize the weights of each neural layer\n",
        "    init.xavier_normal_(self.fc1.weight)\n",
        "    init.xavier_normal_(self.fc2.weight)\n",
        "    init.xavier_normal_(self.fc3.weight)\n",
        "\n",
        "  # Define forward propagation\n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IHV37Ek_jrhR"
      },
      "outputs": [],
      "source": [
        "# Construct the model\n",
        "model = PriceModel(X_train_tensor.shape[1]).to(device)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQWK35FrjrhR"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "caqz0CU-jrhR"
      },
      "outputs": [],
      "source": [
        "batch_size = 15\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-34SgrP1jrhR",
        "outputId": "89a23410-2780-4eed-e9e1-6084bc26a967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/70,RMSE Loss: 189135.8657\n",
            "Epoch 2/70,RMSE Loss: 152136.3865\n",
            "Epoch 3/70,RMSE Loss: 109215.3894\n",
            "Epoch 4/70,RMSE Loss: 100010.4848\n",
            "Epoch 5/70,RMSE Loss: 98200.4141\n",
            "Epoch 6/70,RMSE Loss: 96025.1691\n",
            "Epoch 7/70,RMSE Loss: 95292.0838\n",
            "Epoch 8/70,RMSE Loss: 91538.6097\n",
            "Epoch 9/70,RMSE Loss: 90497.8464\n",
            "Epoch 10/70,RMSE Loss: 88426.0540\n",
            "Epoch 11/70,RMSE Loss: 87146.1998\n",
            "Epoch 12/70,RMSE Loss: 85753.3373\n",
            "Epoch 13/70,RMSE Loss: 83922.5469\n",
            "Epoch 14/70,RMSE Loss: 81861.1919\n",
            "Epoch 15/70,RMSE Loss: 80447.0919\n",
            "Epoch 16/70,RMSE Loss: 78480.1618\n",
            "Epoch 17/70,RMSE Loss: 77371.4589\n",
            "Epoch 18/70,RMSE Loss: 76478.3000\n",
            "Epoch 19/70,RMSE Loss: 75525.0137\n",
            "Epoch 20/70,RMSE Loss: 74706.2489\n",
            "Epoch 21/70,RMSE Loss: 74673.4726\n",
            "Epoch 22/70,RMSE Loss: 73304.4990\n",
            "Epoch 23/70,RMSE Loss: 73513.6243\n",
            "Epoch 24/70,RMSE Loss: 73118.4169\n",
            "Epoch 25/70,RMSE Loss: 72115.7125\n",
            "Epoch 26/70,RMSE Loss: 72523.4076\n",
            "Epoch 27/70,RMSE Loss: 72465.9516\n",
            "Epoch 28/70,RMSE Loss: 72291.9104\n",
            "Epoch 29/70,RMSE Loss: 71771.4923\n",
            "Epoch 30/70,RMSE Loss: 71439.7030\n",
            "Epoch 31/70,RMSE Loss: 71810.8956\n",
            "Epoch 32/70,RMSE Loss: 72077.1596\n",
            "Epoch 33/70,RMSE Loss: 71934.1739\n",
            "Epoch 34/70,RMSE Loss: 71501.3982\n",
            "Epoch 35/70,RMSE Loss: 71474.7447\n",
            "Epoch 36/70,RMSE Loss: 71118.7744\n",
            "Epoch 37/70,RMSE Loss: 71650.9658\n",
            "Epoch 38/70,RMSE Loss: 71627.8480\n",
            "Epoch 39/70,RMSE Loss: 70925.6407\n",
            "Epoch 40/70,RMSE Loss: 71653.3539\n",
            "Epoch 41/70,RMSE Loss: 70721.8238\n",
            "Epoch 42/70,RMSE Loss: 71007.1139\n",
            "Epoch 43/70,RMSE Loss: 70779.0073\n",
            "Epoch 44/70,RMSE Loss: 71295.5021\n",
            "Epoch 45/70,RMSE Loss: 71539.8856\n",
            "Epoch 46/70,RMSE Loss: 70425.7309\n",
            "Epoch 47/70,RMSE Loss: 70909.1331\n",
            "Epoch 48/70,RMSE Loss: 70916.2033\n",
            "Epoch 49/70,RMSE Loss: 70766.2723\n",
            "Epoch 50/70,RMSE Loss: 70494.0131\n",
            "Epoch 51/70,RMSE Loss: 70914.9145\n",
            "Epoch 52/70,RMSE Loss: 70623.0823\n",
            "Epoch 53/70,RMSE Loss: 70090.6358\n",
            "Epoch 54/70,RMSE Loss: 69991.2869\n",
            "Epoch 55/70,RMSE Loss: 70692.3203\n",
            "Epoch 56/70,RMSE Loss: 70239.8675\n",
            "Epoch 57/70,RMSE Loss: 70105.4460\n",
            "Epoch 58/70,RMSE Loss: 70695.7485\n",
            "Epoch 59/70,RMSE Loss: 70639.3754\n",
            "Epoch 60/70,RMSE Loss: 70619.8314\n",
            "Epoch 61/70,RMSE Loss: 69813.5627\n",
            "Epoch 62/70,RMSE Loss: 70426.1578\n",
            "Epoch 63/70,RMSE Loss: 69856.6077\n",
            "Epoch 64/70,RMSE Loss: 70365.0827\n",
            "Epoch 65/70,RMSE Loss: 69753.6773\n",
            "Epoch 66/70,RMSE Loss: 70384.8395\n",
            "Epoch 67/70,RMSE Loss: 70164.4709\n",
            "Epoch 68/70,RMSE Loss: 69450.5245\n",
            "Epoch 69/70,RMSE Loss: 70180.3270\n",
            "Epoch 70/70,RMSE Loss: 69333.7920\n"
          ]
        }
      ],
      "source": [
        "epochs = 70\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # Set the model to training mode\n",
        "  model.train()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  total_samples = 0\n",
        "\n",
        "  # Take a Batch and start training\n",
        "  for X_batch, Y_batch in train_loader:\n",
        "    # Translate the data from the Batch to the GPU\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "    # Zero out the gradient of the previous Batch\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate the output\n",
        "    Y_pred = model(X_batch)\n",
        "    # Calculate the loss\n",
        "    loss = torch.sqrt(criterion(Y_pred, Y_batch))\n",
        "    # Backward propagation\n",
        "    loss.backward()\n",
        "    # Update the weight\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item() * X_batch.size(0)\n",
        "    total_samples += X_batch.size(0)\n",
        "\n",
        "  # Calculate the accuracy for each epoch\n",
        "  epoch_loss = running_loss / total_samples\n",
        "  print(f'Epoch {epoch+1}/{epochs},RMSE Loss: {epoch_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRHxAYsljrhS"
      },
      "source": [
        "## Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekNZuNH4jrhS",
        "outputId": "e3ddfb0a-b41f-4163-dc45-43521eab423f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test RMSE Loss: 71318.9317\n"
          ]
        }
      ],
      "source": [
        "# Switch to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "test_loss = 0.0\n",
        "total_samples = 0\n",
        "\n",
        "# Turn off PyTorch's gradient calculation\n",
        "with torch.no_grad():\n",
        "  # Take a Batch and start testing\n",
        "  for X_batch, Y_batch in test_loader:\n",
        "    # Transform the data from CPU into GPU.\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "    # Calculate output values\n",
        "    Y_pred = model(X_batch)\n",
        "\n",
        "    # Calculate model predictions\n",
        "    Y_pred = model(X_batch)\n",
        "    # Calculate loss for the batch\n",
        "    loss = torch.sqrt(criterion(Y_pred, Y_batch))\n",
        "\n",
        "    # Accumulate total loss and sample count\n",
        "    test_loss += loss.item() * X_batch.size(0)\n",
        "    total_samples += X_batch.size(0)\n",
        "\n",
        "# Compute the average loss for the test dataset\n",
        "avg_test_loss = test_loss / total_samples\n",
        "print(f'Test RMSE Loss: {avg_test_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKnZZe5PqIRS"
      },
      "source": [
        "## Prediction Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvggTrANqMky"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    Y_pred = model(X_pred_tensor.to(device))\n",
        "\n",
        "Y_pred_numpy = Y_pred.squeeze().cpu().tolist()\n",
        "\n",
        "submission = pd.DataFrame({'Id': dataset_test['Id'], 'SalePrice': Y_pred_numpy})\n",
        "submission.to_csv('submission_price.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vllzzAAIgi6x",
        "XrMxyXfDhINZ",
        "3m4PBde2j2sh",
        "sI7tpfqB4QFb",
        "4TWsKMM-mq_7",
        "AXlsyviGv6en",
        "3Em6gjj3v6eo",
        "EBi1IwmIv6eo",
        "q6j2XXb9v6eo",
        "yXFpMP-ojrhR",
        "pDL3hvyFjrhR",
        "eQWK35FrjrhR",
        "wRHxAYsljrhS"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}